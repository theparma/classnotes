\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}
Ozet Istatistikleri, Grafikleri

Beklenti (Expectation) 

Bu deger, dagilim $f(x)$'in tek sayilik bir ozetidir. Yani beklenti hesabina
bir taraftan bir dagilim fonksiyonu girer, diger taraftan tek bir sayi
disari cikar. 

Tanim

Surekli dagilim fonksiyonlari icin $E(X)$

$$  E(X) = \int x f(x) dx$$

ayriksal dagilimlar icin

$$ E(X) = \sum_x xf(x) $$

Hesabin, her $x$ degerini onun olasiligi ile carpip topladigina dikkat. Bu
tur bir hesap dogal olarak tum $x$'lerin ortalamasini verecektir, ve
dolayli olarak dagilimin ortalamasini hesaplayacaktir. Ortalama $\mu_x$
olarak ta gosterilebilir.

$E(X)$'in bir tanim olduguna dikkat, yani bu ifade tamamen bizim
yarattigimiz, ortaya cikarttigimiz bir sey, matematigin baz kurallarindan
gelerek turetilen bir kavram degil. 

Notasyonel basitlik icin ustteki toplam / entegral yerine 

$$ = \int x \ dF(x) $$

diyecegiz, bu notasyonel bir kullanim sadece, unutmayalim, reel analizde
$\int x \ dF(x)$'in ozel bir anlami var (hoca tam diferansiyel $dF$'den
bahsediyor). 

Beklentinin taniminin kapsamli / eksiksiz olmasi icin $E(X)$'in
``mevcudiyeti'' icin de bir sart tanimlamak gerekir, bu sart soyle olsun, 

$$ \int_x |x|dF_X(x) < \infty $$

ise beklenti mevcut demektir. Tersi sozkonusu ise beklenti mevcut
degildir. 

Ornek 

$X \sim Unif(-1,3)$ olsun. $E(X) = \int xdF(x) = \int x f_X(x)dx = \frac{
  1}{4} \int _{ -1}^{3} x dx = 1$. 

Ornek 

Cauchy dagiliminin $f_X(x) = \{ \pi (1+x^2) \} ^{-1}$ oldugunu soylemistik. Simdi 
beklentiyi hesaplayalim. Parcali entegral teknigi lazim, $u=x$, 
$dv =
1/1+x^2$ deriz, ve o zaman $v = \tan ^{-1}(x)$ olur, bkz {\em Ters
  Trigonometrik Formuller} yazimiz. Demek ki 

$$ \int |x|dF(x) = \frac{ 2}{\pi} \int _{ 0}^{\infty}\frac{x dx}{1+x^2}  $$

2 nereden cikti? Cunku $|x|$ kullaniyoruz, o zaman sinir degerlerinde
sadece sifirin sagina bakip sonucu ikiyle carpmak yeterli. Bir sabit oldugu
icin $\pi$ ile beraber disari cikiyor. Simdi

$$ \int udv = uv - \int vdu $$
 
uzerinden

$$ = [x \tan ^{-1}(x) ] _{ 0}^{\infty} - \int _{ 0}^{\infty} \tan ^{-1}(x)
dx  = \infty$$

Yani ustteki hesap sonsuzluga gider. O zaman ustteki tanimimiza gore Cauchy
dagiliminin beklentisi yoktur. 

Tanim

$x_1,..,x_n$ verilerini iceren orneklemin (sample) ortalamasi 

$$ \bar{x} = \frac{1}{n} \sum x_i
\mlabel{1}
$$

Dikkat bu orneklemdeki verinin ortalamasi. Hicbir dagilim hakkinda hicbir
faraziye yapmadik. Ayrica tanim kullandik, yani bu ifadenin ne oldugu
tamamen bize bagli. 

Orneklem ortalamasi sadece tek merkezi bir tepesi olan (unimodal)
dagilimlar icin gecerlidir. Eger bu temel varsayim gecerli degilse,
ortalama kullanarak yapilan hesaplar bizi yanlis yollara goturur. Ayrica
bir dagilimi simetrik olup olmadigi da ortalama ya da medyan kullanilip
kullanilmamasi kararinda onemlidir. Eger simetrik, tek tepeli bir dagilim
var ise, ortalama ve medyan birbirine yakin olacaktir. Fakat veri baska
turde bir dagilim ise, o zaman bu iki olcut birbirinden cok farkli
olabilir.


Tanim

$Y$ rasgele degiskeninin varyansi (variance) 

$$ Var(Y) = E((Y-E(Y))^2) $$

Ifadede toplama ve bolme gibi islemler olmadigina dikkat; onun yerine kare
ifadeleri uzerinde beklenti ifadesi var. Yani $Y$'nin beklentisini rasgele
degiskenin kendisinden cikartip kareyi aliyoruz, ve bu islemin $Y$'den
gelen tum zar atislari uzerinden beklentisi bize varyansi veriyor. Bir
rasgele degisken gorunce onun yerine ``dagilimdan uretilen sayi'' dusunmek
faydalidir, ki bu gercek dunya sartlarindan (ve buyuk miktarda olunca) veri
noktalarini temsil eder. 

Tanim

$y_1,..,y_n$ ornekleminin varyansi 

$$ \frac{1}{n} \sum (y_i - \bar{y})^2
\mlabel{2}
$$

Standart sapma veri noktalarin "ortalamadan farkinin ortalamasini"
verir. Tabii bazen noktalar ortalamanin altinda, bazen ustunde olacaktir,
bizi bu negatiflik, pozitiflik ilgilendirmez, biz sadece farkla
alakaliyiz. O yuzden her sapmanin karesini aliriz, bunlari toplayip nokta
sayisina boleriz. Ilginc bir cebirsel islem sudur. Eger $\bar{y}$ tanimini
ustteki formule sokarsak,

$$ = \frac{ 1}{n} \sum_i y_i^2 + \frac{ 1}{n} \sum_i m^2 - \frac{ 2}{n} \sum_i y_i\bar{y}  $$

$$ = \frac{ 1}{n} \sum_i y_i^2 + \frac{ \bar{y}^2n}{n} - \frac{ 2\bar{y}n}{n}\bar{y} $$

$$ = \frac{ 1}{n} \sum_i y_i^2 +  \bar{y}^2 - 2\bar{y}^2 $$

$$ = \frac{ 1}{n} \sum_i y_i^2 - \bar{y}^2 $$

Bu arada standard sapma varyansin karesidir, ve biz kareli olan versiyon
ile calismayi tercih ediyoruz. Niye? Cunku o zaman veri noktalarinin ve
yayilma olcusunun birimleri birbiri ile ayni olacak. Eger veri setimiz bir
alisveris sepetindeki malzemelerin lira cinsinden degerleri olsaydi,
varyans bize sonucu "karekok lira" olarak verecekti ve bunun pek anlami
olmayacakti.

Kovaryans ve Korelasyon (Covariance and Correlation)

Iki veya daha fazla boyutun arasindaki iliskileri gormek veri analizinde
onemli bir yer tutar. Yontemlerden birisi verideki mumkun her ikili
iliskiyi grafiksel olarak gostermektir. Pandas \verb!scatter_matrix! bunu
yapabilir. Iris veri seti uzerinde gorelim, her boyut hem y-ekseni hem
x-ekseninde verilmis, iliskiyi gormek icin eksende o boyutu bulup kesisme
noktalarindaki grafige bakmak lazim. 

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('iris.csv')
df = df.ix[:,0:4]
pd.scatter_matrix(df)
plt.savefig('stat_summary_01.png')
\end{minted}

\includegraphics[height=8cm]{stat_summary_01.png}


Iliski oldugu zaman o iliskiye tekabul eden grafikte ``duz cizgiye benzer''
bir goruntu olur, demek ki degiskenlerden biri artinca oteki de artiyor,
azalinca oteki de azaliyor demektir. Eger ilinti yok ise bol gurultulu, ya
da yuvarlak kureye benzer bir sekil cikar. Ustteki grafige gore yaprak
genisligi (petal width) ile yaprak boyu (petal length) arasinda bir iliski
var. 

Tanim

$X,Y$ rasgele degiskenlerin arasindaki kovaryans,

$$ Cov(X,Y) = E(X-E(X))(Y-E(Y)) $$

Yani hem $X$ hem $Y$'nin beklentilerinden ne kadar saptiklarini her veri
ikilisi icin, cikartarak tespit ediyoruz, daha sonra bu farklari birbiriyle
carpiyoruz, ve beklentisini aliyoruz (yani tum olasilik uzerinden ne
olacagini hesapliyoruz). Carpimlarin ilginc ozelligi sudur, arti deger arti
degerle carpilinca arti, eksi-eksi arti verir, eksi-arti eksi
verir. Pozitif sonuc pozitif korelasyon, negatif ise tersi sekilde ilinti
var anlamina gelir. 

Ayri ayri $X,Y$ degiskenleri yerine cok boyutlu $X$ kullanirsak, ki
boyutlari $m,n$ olsun yani $m$ veri noktasi ve $n$ boyut (ozellik, oge)
var, tanimi soyle ifade edebiliriz,

$$ \Sigma = Cov(X) = E((X-E(X))^T(X-E(X))) $$

Verisel Kovaryans (Empirical Covariance) 

Tanim

$$ S = \frac{1}{n} (X-E(X))^T(X-E(X))) $$

Pandas ile \verb!cov! cagrisi bu hesabi hizli bir sekilde yapar,

\begin{minted}[fontsize=\footnotesize]{python}
print df.cov()
\end{minted}

\begin{verbatim}
              Sepal Length  Sepal Width  Petal Length  Petal Width
Sepal Length      0.685694    -0.039268      1.273682     0.516904
Sepal Width      -0.039268     0.188004     -0.321713    -0.117981
Petal Length      1.273682    -0.321713      3.113179     1.296387
Petal Width       0.516904    -0.117981      1.296387     0.582414
\end{verbatim}

Eger kendimiz bu hesabi yapmak istersek,

\begin{minted}[fontsize=\footnotesize]{python}
means = df.mean()
n = df.shape[0]
df2 = df.apply(lambda x: x - means, axis=1)
print np.dot(df2.T,df2) / n
\end{minted}

\begin{verbatim}
[[ 0.68112222 -0.03900667  1.26519111  0.51345778]
 [-0.03900667  0.18675067 -0.319568   -0.11719467]
 [ 1.26519111 -0.319568    3.09242489  1.28774489]
 [ 0.51345778 -0.11719467  1.28774489  0.57853156]]
\end{verbatim}

Medyan ve Yuzdelikler (Percentile)

Ustteki hesaplarin cogu sayilari toplayip, bolmek uzerinden yapildi. Medyan
ve diger yuzdeliklerin hesabi (ki medyan 50. yuzdelige tekabul eder) icin
eldeki tum degerleri "siraya dizmemiz" ve sonra 50. yuzdelik icin
ortadakine bakmamiz gerekiyor. Mesela eger ilk 5. yuzdeligi ariyorsak ve
elimizde 80 tane deger var ise, bastan 4. sayiya / vektor hucresine / ogeye
bakmamiz gerekiyor. Eger 100 eleman var ise, 5. sayiya bakmamiz gerekiyor,
vs.

Bu siraya dizme islemi kritik. Kiyasla ortalama hesabi hangi sirada olursa
olsun, sayilari birbirine topluyor ve sonra boluyor. Zaten ortalama ve
sapmanin istatistikte daha cok kullanilmasinin tarihi sebebi de aslinda bu;
bilgisayar oncesi cagda sayilari siralamak (sorting) zor bir isti. Bu
sebeple hangi sirada olursa olsun, toplayip, bolerek hesaplanabilecek
ozetler daha makbuldu. Fakat artik siralama islemi kolay, ve veri setleri
her zaman tek tepeli, simetrik olmayabiliyor. Ornek veri seti olarak unlu
\verb!dellstore2! tabanindaki satis miktarlari kullanirsak,

\begin{minted}[fontsize=\footnotesize]{python}
print np.mean(data)
\end{minted}

\begin{verbatim}
213.948899167
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.median(data)
\end{minted}

\begin{verbatim}
214.06
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.std(data)
\end{minted}

\begin{verbatim}
125.118481954
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.mean(data)+2*np.std(data)
\end{minted}

\begin{verbatim}
464.185863074
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.percentile(data, 95)
\end{minted}

\begin{verbatim}
410.4115
\end{verbatim}

Goruldugu gibi uc nokta hesabi icin ortalamadan iki sapma otesini
kullanirsak, 464.18, fakat 95. yuzdeligi kullanirsak 410.41 elde
ediyoruz. Niye? Sebep ortalamanin kendisi hesaplanirken cok uc
degerlerin toplama dahil edilmis olmasi ve bu durum, ortalamanin
kendisini daha buyuk seviyeye dogru itiyor. Yuzdelik hesabi ise sadece
sayilari siralayip belli bazi elemanlari otomatik olarak uc nokta
olarak addediyor.

Box Whisker Grafikleri

Tek boyutlu bir verinin dagilimini gormek icin Box ve Whisker grafikleri
faydali araclardir; medyan (median), dagilimin genisligini ve siradisi
noktalari (outliers) acik sekilde gosterirler. Isim nereden geliyor? Box
yani kutu, dagilimin agirliginin nerede oldugunu gosterir, medyanin
sagindada ve solunda olmak uzere iki ceyregin arasindaki kisimdir, kutu
olarak resmedilir. Whiskers kedilerin biyiklarina verilen isimdir, zaten
grafikte birazcik biyik gibi duruyorlar. Bu uzantilar medyan noktasindan
her iki yana kutunun iki kati kadar uzatilir sonra verideki "ondan az olan
en buyuk" noktaya kadar geri cekilir. Tum bunlarin disinda kalan veri ise
teker teker nokta olarak grafikte basilir. Bunlar siradisi (outlier)
olduklari icin daha az olacaklari tahmin edilir.

BW grafikleri iki veriyi dagilimsal olarak karsilastirmak icin
birebirdir. Mesela Larsen and Marx adli arastirmacilar cok az veri
iceren Quintus Curtius Snodgrass veri setinin degisik oldugunu
ispatlamak icin bir suru hesap yapmislardir, bir suru matematiksel
isleme girmislerdir, fakat basit bir BW grafigi iki setin farkliligini
hemen gosterir.

BW grafikleri iki veriyi dagilimsal olarak karsilastirmak icin
birebirdir. Mesela Larsen and Marx adli arastirmacilar cok az veri
iceren Quintus Curtius Snodgrass veri setinin degisik oldugunu
ispatlamak icin bir suru hesap yapmislardir, bir suru matematiksel
isleme girmislerdir, fakat basit bir BW grafigi iki setin farkliligini
hemen gosterir.

Python uzerinde basit bir BW grafigi 

\begin{minted}[fontsize=\footnotesize]{python}
spread= rand(50) * 100
center = ones(25) * 50
flier_high = rand(10) * 100 + 100
flier_low = rand(10) * -100
data =concatenate((spread, center, flier_high, flier_low), 0)
plt.boxplot(data)
plt.savefig('05_03.png')
\end{minted}

\includegraphics[height=6cm]{05_03.png}

Bir diger ornek Glass veri seti uzerinde

\begin{minted}[fontsize=\footnotesize]{python}
data = loadtxt("glass.data",delimiter=",")
head = data[data[:,10]==7]
tableware = data[data[:,10]==6]
containers = data[data[:,10]==5]

print head[:,1]

data =(containers[:,1], tableware[:,1], head[:,1])

plt.yticks([1, 2, 3], ['containers', 'tableware', 'head'])

plt.boxplot(data,0,'rs',0,0.75)
plt.savefig('05_04.png')
\end{minted}

\begin{verbatim}
[ 1.51131  1.51838  1.52315  1.52247  1.52365  1.51613  1.51602  1.51623
  1.51719  1.51683  1.51545  1.51556  1.51727  1.51531  1.51609  1.51508
  1.51653  1.51514  1.51658  1.51617  1.51732  1.51645  1.51831  1.5164
  1.51623  1.51685  1.52065  1.51651  1.51711]
\end{verbatim}

\includegraphics[height=6cm]{05_04.png}

Parametre Tahmin Ediciler (Estimators) 

Maksimum Olurluk (maximum likelihood) kavramini kullanarak ilginc bazi
sonuclara erismek mumkun; bu sayede dagilim fonksiyonlari ve veri arasinda
bazi sonuclar elde edebiliriz. Maksimum olurluk nedir? MO ile verinin her
noktasi teker teker olasilik fonksiyonuna gecilir, ve elde edilen olasilik
sonuclarinin birbiri ile carpilir. Cogunlukla formul icinde bilinmeyen
bir(kac) parametre vardir, ve bu carpim sonrasi, icinde bu parametre(ler)
olan yeni bir formul ortaya cikar. Bu nihai formulun kismi turevi alinip
sifira esitlenince cebirsel bazi teknikler ile bilinmeyen parametre
bulunabilir. Bu sonuc eldeki veri baglaminda en mumkun (olur) parametre
degeridir. Oyle ya, mesela Gaussian $N(10,2)$ dagilimi var ise, 60,90 gibi
degerlerin ``olurlugu'' dusuktur. Gaussin uzerinde ornek,

$$ f(x;\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} 
\exp \bigg\{ - \frac{1}{2\sigma^2}(x-\mu)^2  \bigg\}
, \ x \in \mathbb{R}
$$
 
Carpim sonrasi

$$ f(x_1,..,x_n;\mu,\sigma) = 
\prod \frac{1}{\sigma\sqrt{2\pi}} 
\exp \bigg\{ - \frac{1}{2\sigma^2}(x_i-\mu)^2  \bigg\}
$$

$$ =
\frac{(2\pi)^{-n/2}}{\sigma^n}
\exp \bigg\{ - \frac{\sum (x_i-\mu)^2}{2\sigma^2}  \bigg\}
$$

Ustel kisim $-n/2$ nereden geldi? Cunku bolen olan karekoku uste cikardik,
boylece $-1/2$ oldu, $n$ cunku $n$ tane veri noktasi yuzunden formul $n$
kere carpiliyor. Veri noktalari $x_i$ icinde. Eger log, yani $\ln$ alirsak
$\exp$'den kurtuluruz, ve biliyoruz ki log olurlugu maksimize etmek normal
olurlugu maksimize etmek ile ayni seydir, cunku $\ln$ transformasyonu
monoton bir transformasyondur. Ayrica olurluk icbukeydir (concave) yani
kesin tek bir maksimumu vardir. 

$$ \ln f = -\frac{1}{2} n \ln (2\pi) 
- n \ln \sigma - 
\frac{\sum (x_i-\mu)^2}{2\sigma^2}  
$$

Turevi alip sifira esitleyelim

$$ \frac{\partial (\ln f)}{\partial \mu} =
\frac{\sum (x_i-\mu)^2}{2\sigma^2}   = 0 
$$

$$ \hat{\mu} = \frac{\sum x_i }{n} $$


Bu sonuc (1)'deki formul, yani orneklem ortalamasi ile ayni! Fakat buradan
hemen bir baglantiya ziplamadan once sunu hatirlayalim - orneklem
ortalamasi formulunu {\em biz} tanimladik. ``Tanim'' diyerek bir ifade
yazdik, ve budur dedik. Simdi sonradan, verinin dagiliminin Gaussian
oldugunu farzederek, bu verinin mumkun kilabilecegi en optimal parametre
degeri nedir diye hesap ederek ayni formule eristik, fakat bu bir anlamda
bir guzel raslanti oldu.. Daha dogrusu bu aynilik Gaussian / Normal
dagilimlarinin ``normalligi'' ile alakali muhakkak, fakat ornekleme
ortalamasi hicbir dagilim faraziyesi yapmiyor, herhangi bir dagilimdan
geldigi bilinen ya da bilinmeyen bir veri uzerinde kullanilabiliyor (ise
yaramayabilir de tabii). Bunu unutmayalim. Istatistikte matematigin
lakaytlasmasi (sloppy) kolaydir, o sebeple neyin tanim, neyin hangi
faraziyeye gore optimal, neyin nufus (population) neyin orneklem (sample)
oldugunu hep hatirlamamiz lazim.

Devam edelim, maksimum olurluk ile $\hat{\sigma}$ hesaplayalim,

$$ \frac{\partial (\ln f)}{\partial \sigma} =
-\frac{n}{\sigma} + \frac{\sum (x_i-\mu)^2}{2\sigma^3}   = 0 
$$

Cebirsel birkac duzenleme sonrasi ve $\mu$ yerine yeni hesapladigimiz
$\hat{\mu}$ kullanarak,

$$ \hat{\sigma}^2 = \frac{\sum (x_i-\hat{\mu})^2}{n} $$

Bu da orneklem varyansi ile ayni! 

Büyük Sayýlar Kanunu (Law of Large Numbers)

Olasýlýk kuramýnda önemli matematiksel bir denklem, büyük sayýlar
kanunudur. Bu kanun, orneklem (sample) ile rasgele degiskenler, yani
matematiksel olasilik dagilimlari olan dunya arasinda bir baglanti gorevi
gorur. 

Kanun kabaca bildiðimiz günlük bir gerçeðin matematiksel ispatýdýr da
denebilir. Yazý-tura atarken yazý çýkma ihtimalinin 1/2 olduðunu biliyoruz
(cunku zar atisini bir dagilim gibi goruyoruz). Herhalde çoðumuz da bu
yazý-tura iþleminin "bir çok kere" tekrarlandýðý durumda, toplam sonucun
aþaðý yukarý yarýsýnýn yazý olacaðýný tahmin biliyoruz.

Matematiksel olarak, farzedelim ki her yazý-tura atýþý bir deney olsun. Her
ayrý deneyin sonucu $X_1, X_2...X_n$ olarak rasgelen deðiþkenlerle
tanýmlanmýþ olsun, bu degiskenlerin dagilimi ayni (cunku ayni zar), ama
birbirlerinden bagimsizlar (cunku her deney digerinden
alakasiz). Deðiþkenlerin sonucu 1 ya da 0 deðeri taþýyacak, Yazý=1,
Tura=0. 

Buyuk Sayilar Kanunu tum bu deney sonuclarinin, yani rasgele degiskenlerin
averaji alinirsa, elde edilen sonucun $X_i$'lerin (ayni olan) beklentisine
yaklasacaginin soyler, yani $n$ büyüdükçe $\bar{X}_n$'in 1/2'ye
yaklaþtýðýný ispatlar, yani $E[X_i] = 1/2$ degerine.

Formulsel olarak, herhangi bir $\epsilon > 0$ icin,

$$ \lim_{n \to \infty} P(|\bar{X} - \mu| \le \epsilon) = 1$$
ya da

 $$ \lim_{n \to \infty} P(|\bar{X}_n-\mu| > \epsilon) = 0 $$
ya da 

 $$ P(|\bar{X}_n-\mu| > \epsilon) \rightarrow 0 $$


Burada ne soylendigine dikkat edelim, $X_i$ dagilimi {\em ne olursa olsun},
yani ister Binom, ister Gaussian olsun, {\em orneklem} uzerinden hesaplanan
sayisal ortalamanin (empirical mean) formulsel olasilik beklentisine
yaklastigini soyluyoruz! $X_i$'ler en absurt dagilimlar olabilirler, bu
dagilimlarin fonksiyonu son derece cetrefil, tek tepeli (unimodal) bile
olmayabilir, o formuller uzerinden beklenti icin gereken entegralin belki
analitik cozumu bile mevcut olmayabilir! Ama yine de ortalama, o
dagilimlarin beklentisine yaklasacaktir.  Istatistik ile olasilik teorisi
arasindaki cok onemli bir baglanti bu.

Sonuc sasirtici, fakat bir ek daha yapalim, sezgisel (intuite) olarak
bakarsak aslinda sonuc cok sasirtici olmayabilir. Niye? Diyelim ki
orneklem $N(\mu,\sigma^2)$ seklinde bir Normal dagilim. Bu durumda
orneklemdeki veri noktalarinin $\mu$'ya yakin degerler olmasini beklemek
mantikli olmaz mi? Cunku bu dagilim ``zar atinca'' onu $\mu,\sigma^2$'e
gore atacak. Orneklemi zar atisi sonuclari olarak gordugumuze gore elde
edilen verilerin bu sekilde olacagi sasirtici olmamali. Ve bu zar
atislarinin ortalamasinin, son derece basit bir aritmetik bir islemle
hesaplaniyor olsa bile, $\mu$'ye yaklasmasi normal olmali. 

Bu arada, bu argumana tersten bakarsak Monte Carlo entegralinin niye
isledigine erismis oluyoruz (bkz {\em Monte Carlo, Entegraller, MCMC}
yazisi). 

Ozellikle orneklem ile genel nufus (population) arasinda kurulan baglantiya
dikkat edelim. Istatigin onemli bir bolumunun bu baglanti oldugu
soylenebilir. Orneklem, bilmedigimiz ama genel nufusu temsil eden bir
dagilimla ayni dagilima sahip olan $X_i$'dir dedik, ve bu ayniliktan ve
bagimsizliktan yola cikarak bize genel nufus hakkinda bir ipucu saglayan
bir kanun gelistirdik (ve birazdan ispatlayacagiz).

Ispata baþlayalým.

$X_1,X_2,..,X_n$ bagimsiz degiskenler olsun. 

$$ E(X_i) = \mu $$

$$ Var(X_i) = \sigma $$

$$ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i  $$


$\bar{X}_n$ de bir rasgele deðiþkendir, çünku $\bar{X}_n$ deðiþkeni her
$X_i$ daðýlýmýyla alakali.

Ýspat devam etmek için, $\bar{X}_n$ daðýlýmýnýn beklentisini bulmamýz gerekiyor. 

$$ E(\bar{X}_n) = E(\frac{1}{n} \sum_{i=1}^n X_i)  $$

E dogrusal bir islec (linear operator) oldugu icin disaridan iceri dogru
nufuz eder. 

$$ = \frac{1}{n} \sum_{i=1}^n E(X_i) = \frac{1}{n}n\mu $$

$$ = \mu $$

Dikkat edelim, bu {\em ortalamanin} beklentisi, ortalamanin kendisinin
hangi degere yaklasacagini hala gostermiyor. Eger oyle olsaydi isimiz
bitmis olurdu :) Daha yapacak cok is var.

Simdi $\bar{X}_n$ daðýlýmýnýn standart sapmasýný da bulalým. Diger bir
olasilik kuramina gore

$$ Y = a + bX $$

$$ Var(Y) = b^2Var(X) $$

oldugunu biliyoruz. O zaman,

$$ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i  $$

$$ Var(\bar{X}_n) = Var(\frac{1}{n}\sum_{i=1}^nX_i) = 
\frac{1}{n^2}\sum_{i=1}^n Var(X_i)
$$

$$ Var(\bar{X}_n) = \frac{1}{n^2}\sum_{i=1}^n \sigma^2 = 
\frac{1}{n^2}n\sigma^2 = \frac{\sigma^2}{n} 
$$

Artýk Çebiþev kuramýný kullanmaya hazýrýz. Ispatlamaya calistigimiz neydi?
$n \rightarrow \infty$ iken,

 $$ P(|\bar{X}_n-\mu| > \epsilon) \rightarrow 0 $$

Cebisev'den

$$ P(|\bar{X}_n-\mu| > \epsilon) \le \frac{Var(\bar{X}_n)}{\epsilon^2} $$

$$ P(|\bar{X}_n-\mu| > \epsilon) \le \frac{\sigma^2}{n\epsilon^2}
\rightarrow 0 $$

$\sigma^2 / n\epsilon^2$'in sifira gitmesi normal cunku n sonsuza gidiyor.

Peki $P(|\bar{X}_n-\mu| > \epsilon)$'nin sifira gittigini gosterdik mi? 

$\sigma^2 / n\epsilon^2$'nin sifira gittigini gosterdik. $\sigma^2 /
n\epsilon^2$ de $P(|\bar{X}_n-\mu| > \epsilon)$'den 
buyuk olduguna gore, demek ki o da sifira iner.

Çebiþev Eþitsizliði

Olasýlýk matematiðinde, büyük sayýlar kuramý adýnda anýlan ve olasýlýk
matematiðinin belkemiðini oluþturan kuramý ispatlamak için, diðer bir kuram
olan Çebiþev eþitsizliðini de anlamamýz gerekiyor. Çebiþev eþitsizliði bir
rasgele deðiþken, onun ortalamasý (beklentisi) ve herhangi bir sabit sayý
arasýndaki üçlü arasýnda bir 'eþitsizlik' baðlantýsý kurar, ve bu baðlantý
diðer olasýlýk iþlemlerimizde ispat verisi olarak iþimize yarar.

Teori: Herhangi bir $t$ deðeri için, 

\[ P(|X-\mu| > t) \le \frac{\sigma^2}{t^2} \]

Ýspata baþlayalým. Entegral ile olasýlýk hesabý yapmak için bize bir $x$
uzayý lazým.

\[ \mathbb{R} = {x: |x-\mu| > t} \]

Yani $\mathbb{R}$ uzayý, $x$ ile ortalamasýnýn farkýnýn, $t$'den büyük olduðu bütün
sayýlarýn kümesidir.

O zaman, 

\[ P(|X-\mu| > t) = \int_R f(x)dx \]

Dikkat edelim $P(..)$ içindeki formül, küme tanýmý ile ayný. O yüzden $P()$
hesabý ortada daha olmayan, ama varolduðu kesin bir daðýlým fonksiyonu
tanýmlamýþ da oluyor. Buna $f(x)$ deriz. $P()$'in, $f(x)$ fonksiyonunun $R$
üzerinden entegral olduðunu olasýlýða giriþ dersinden bilmemiz lazým. 

Eger $x \in R$ dersek o zaman

\[ \frac{|x-\mu|^2}{t^2} \ge 1 \]

t'nin denkleme bu þekilde nereden geldiði þaþkýnlýk yaratabilir. Daha önce
tanýmlanan þu ibareye dikkat edelim, $x: |x-u| > t$ diye belirtmiþtik. Bu
ifadeyi deðiþtirerek, yukarýdaki denkleme gelebiliriz.

Devam edersek, elimizdeki 1'den büyük bir deðer var. Bu deðeri kullanarak,
aþaðýdaki tanýmý yapmamýz doðru olacaktýr.

\[ \int_R f(x)dx \le \int_R \frac{(x-\mu)^2}{t^2}f(x)dx \le
\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x)dx 
 \]


Ortadaki entegral niye birinci entegralden büyük? Çünkü ortadaki
entegraldeki $f(x)dx$ ibaresinden önce gelen kýsmýn, her zaman 1'den büyük
olacaðýný belirttiðimize göre, ikinci entegralin birinciden büyük olmasý
normaldir, cunku birinci entegral $f(x)$ olasilik dagilimina bagli,
entegral ise bir alan hesabidir ve olasilik dagilimlarinin sonsuzlar
arasindaki entegrali her zaman 1 cikar, kaldi ki ustteki $x$'in uzayini
daha da daralttik. 

Evet...Üçüncü entegral ispata oldukça yaklaþtý aslýnda. Standart sapma
iþaretini hala ortada göremiyoruz, fakat son entegraldeki ibare standart
sapma deðerini zaten içeriyor. Önce daha önceki olasýlýk natematiði
bilgimize dayanarak, standart sapmanýn tanýmýný yazýyoruz. Dikkat edelim,
bu ibare þu anki ispatýmýz dahilinden deðil, haricinden önceki bilgimize
dayanarak geldi. Standart sapmanýn tanýmý þöyledir.

\[ \sigma^2 = \int_{-\infty}^{\infty} (x-\mu)^2f(x)dx \]

O zaman

\[ \frac{\sigma^2}{t^2} = \int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x)dx \]

yani

\[ \int_R f(x)dx \le \frac{\sigma^2}{t^2} = 
\int_{-\infty}^{\infty} \frac{(x-\mu)^2}{t^2}f(x)dx
\]

ki $\int_R f(x)dx$ zaten $P(|X-\mu| > t)$ olarak tanimlanmisti. 

Kaynaklar

[1] \url{http://mathworld.wolfram.com/MaximumLikelihood.html}


\end{document}
