\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}
Kalici CD (Persistent Contrastive Divergence -PCD-)

Kisitli Boltzman Makinalari (RBM) yazisinda gosterilen egitim CD
(contrastive divergence) uzerinden idi. Amac alttaki formulde, ozellikle
eksiden sonraki terimi yaklasiksal olarak hesaplamaktir. 

$$ \sum_{n=1}^{N}  <y_iy_j>_{P(h|x^n;W)} - <y_iy_j>_{P(x,h;W)} $$

Bu terime basinda eksi oldugu icin negatif parcaciklar (negative particles)
ismi de veriliyor. 

Simdi RBM'de gordugumuz CD'yi hatirlayalim, CD bir tur ``tek adimlik Gibbs
orneklemesi'' yapiyordu; bu tek adim ornekleme sonrasinda bir sonraki adim
oncesi, veri, tekrar baslangic noktasi olarak zincire veriliyordu. Yani her
CD adiminin baslangici illa ki verinin kendisi olacaktir. Bu usul Gibbs'in
veriden uzaklasma sansi cok azdir. Fakat cogu ilginc yapay ogrenim verisi
çokdorukludur (multimodal), optimizasyon baglaminda dusunulurse birden
fazla tepe (ya da cukur) noktasi icerir. Eger eldeki veri, egitimi bu
noktalara yeterince kanalize edemiyorsa o noktalar ogrenilmemis olur. Bazen
verinin (bile) soylediginden degisik yonleri gezebilen bir prosedur bu
cokdoruklu alani gezmesi acisindan daha basarili olabilecektir.

PCD bu eksikleri duzeltmeye cabalar. PCD'ye gore modelden gelen ``negatif
parcaciklarin'' orneklemesi arka planda, kendi baslarina ilerler, ve bu
zincir hicbir zaman veriye, ya da baska bir seye set edilmez (hatta
zincirin baslangic noktasi bile veriden alakasiz olarak, rasgele
secilir). Bu yonteme gore $h^0,x^0, h^1, x^1, ...$ uretimi neredeyse
tamamen ``kapali devre'' kendi kendine ilerleyen bir surec olacaktir. Diger
yanda pozitif parcaciklar veriden geliyor (ve tabii ki her gradyan adimi
sonrasi degisen $W$ hem pozitif hem negatif parcaciklari etkiler), ve bu
al/ver iliskisi, hatta bir bakima model ile verinin kapismasinin PCD'yi
daha avantajli hale getirdigi iddia edilir, ki PCD, CD'den genellikle daha
iyi ogrenim saglar [5].

CD'ye kiyasla PCD'nin Gibbs ya da genel olarak MCMC orneklemesinin
prensibine daha yakin durdugu iddia edilebilir, cunku PCD ile bir orneklem
zinciri kesintisiz olarak devam ettirilir. 

\inputminted[fontsize=\footnotesize]{python}{rbmp.py}

Ustte gorulen kod daha once RBM icin kullanilan kodla benzesiyor, sadece
\verb!fit! degisik, ve \verb!_fit! eklendi. Bu kodda miniparca (minibatch)
kavrami da var, her gradyan adimi ufak verinin mini parcalari uzerinden
atilir. Bu parcalar hakikaten ufak, mesela 10 ila 100 satirlik veri
arasindadirlar ve bu ilginc bir durumu ortaya cikartir, ozellikle negatif
parcaciklar icin, ki bu parcaciklar $W$ baglantisi haricinde kendi baslarina
ilerler, cok az veri noktasi ile islem yapabilmektedirler.

Metot \verb!fit! icinde \verb!self.h_samples_! degiskenine dikkat, bu
degisken PCD'nin ``kalici'' olmasini saglar, her \verb!_fit! cagri sonrasi
negatif parcacik orneklemesi \verb!self.h_samples_! 'in biraktigi yerden
baslar.


RBM icin kullandigimiz ayni veri seti uzerine k-katlama ile test edelim,

\inputminted[fontsize=\footnotesize]{python}{test_rbmkfold.py}

\begin{minted}[fontsize=\footnotesize]{python}
! python test_rbmkfold.py
\end{minted}

\begin{verbatim}
0.989898989899
\end{verbatim}

Daha cetrefil bir veri seti MNIST veri setine [2] bakalim. Veri 28x28
boyutunda ikisel veri olarak kodlanmis rakamlarin el yazisindan alinmis
resimlerini icerir. Veri seti unlu cunku Derin Ogrenim'in ilk buyuk
basarilari bu veri seti uzerinde paylasildi. MNIST'i aldiktan sonra egitim
/ test kisimlarinin ilk 1000 tanesi uzerinde algoritmamizi kullanirsak, tek
komsulu KNN (yani 1-NN) yuzde 85.4 basari sonucunu verir. Alttaki
parametreler uzerinden PCD ile RBM'in basarisi yuzde 86 olacaktir.

\inputminted[fontsize=\footnotesize]{python}{test_mnist.py}

Kaynaklar

[1] \url{http://videolectures.net/icml09_tieleman_ufw/}

[2] \url{http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz}

[3] Bengio, Y., {\em Learning Deep Architectures for AI}

[4] Larochelle, H., Neural Networks class,
\url{https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBHd}

[5] Murphy, K. {\em Machine Learning A Probabilistic Perspective}

\end{document}
